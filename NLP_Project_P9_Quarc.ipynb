{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Project_P9_Quarc.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8iHRg9fd2jB2",
        "ityk-IlQI9U2",
        "FKLlxwetKOFg",
        "M3poxFfBJ_G1",
        "NT9GhmWqJ1jp",
        "eYrjMyzvJjpZ",
        "7HzvLF5eJZEG",
        "qZKz9ghvDHw8"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-ePSR15KER8"
      },
      "source": [
        "# **Project-P9 for CS60075: Natural Language Processing**\n",
        "## Automated query processing from passages\n",
        "Using rule based Quarc Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z_HRN9UKKGa"
      },
      "source": [
        "# Team Members (Group-3): \n",
        "Jaisaikrishnan\n",
        "\n",
        "Shivam Bhosale\n",
        "\n",
        "Mayank agrawal\n",
        "\n",
        "\n",
        "Arundhuti Nuskar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09fRFkEZ2r4z"
      },
      "source": [
        "# **Importing libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J40l16LYTMr8",
        "outputId": "86604379-dfac-4746-8a4b-e208ac04480b"
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "#Stop words for question and sentence...\n",
        "question_stop_words = [w for w in stop_words if not w in [\"this\"]]\n",
        "sentence_stop_words = [w for w in stop_words if not w in [\"so\", \"because\", \"from\"]]\n",
        "\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "porter_stemmer  = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RnVV8-S2bC6"
      },
      "source": [
        "# **Reading the corpus**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR9ZUvkiLfu2",
        "outputId": "b3f490b0-9cce-4583-f116-0c6730513204"
      },
      "source": [
        "corpus = open(\"/content/corpus.txt\",\"r\").read()\n",
        "\n",
        "print(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.\n",
            "Following the Cretaceous–Paleogene extinction event, the extinction of the dinosaurs and the wetter climate may have allowed the tropical rainforest to spread out across the continent. From 66–34 Mya, the rainforest extended as far south as 45°. Climate fluctuations during the last 34 million years have allowed savanna regions to expand into the tropics. During the Oligocene, for example, the rainforest spanned a relatively narrow band. It expanded again during the Middle Miocene, then retracted to a mostly inland formation at the last glacial maximum. However, the rainforest still managed to thrive during these glacial periods, allowing for the survival and evolution of a broad diversity of species.\n",
            "During the mid-Eocene, it is believed that the drainage basin of the Amazon was split along the middle of the continent by the Purus Arch. Water on the eastern side flowed toward the Atlantic, while to the west water flowed toward the Pacific across the Amazonas Basin. As the Andes Mountains rose, however, a large basin was created that enclosed a lake; now known as the Solimões Basin. Within the last 5–10 million years, this accumulating water broke through the Purus Arch, joining the easterly flow toward the Atlantic.\n",
            "There is evidence that there have been significant changes in Amazon rainforest vegetation over the last 21,000 years through the Last Glacial Maximum (LGM) and subsequent deglaciation. Analyses of sediment deposits from Amazon basin paleolakes and from the Amazon Fan indicate that rainfall in the basin during the LGM was lower than for the present, and this was almost certainly associated with reduced moist tropical vegetation cover in the basin. There is debate, however, over how extensive this reduction was. Some scientists argue that the rainforest was reduced to small, isolated refugia separated by open forest and grassland; other scientists argue that the rainforest remained largely intact but extended less far to the north, south, and east than is seen today. This debate has proved difficult to resolve because the practical limitations of working in the rainforest mean that data sampling is biased away from the center of the Amazon basin, and both explanations are reasonably well supported by the available data.\n",
            "NASA's CALIPSO satellite has measured the amount of dust transported by wind from the Sahara to the Amazon: an average 182 million tons of dust are windblown out of the Sahara each year, at 15 degrees west longitude, across 1,600 miles (2,600 km) over the Atlantic Ocean (some dust falls into the Atlantic), then at 35 degrees West longitude at the eastern coast of South America, 27.7 million tons (15%) of dust fall over the Amazon basin, 132 million tons of dust remain in the air, 43 million tons of dust are windblown and falls on the Caribbean Sea, past 75 degrees west longitude.\n",
            "For a long time, it was thought that the Amazon rainforest was only ever sparsely populated, as it was impossible to sustain a large population through agriculture given the poor soil. Archeologist Betty Meggers was a prominent proponent of this idea, as described in her book Amazonia: Man and Culture in a Counterfeit Paradise. She claimed that a population density of 0.2 inhabitants per square kilometre (0.52/sq mi) is the maximum that can be sustained in the rainforest through hunting, with agriculture needed to host a larger population. However, recent anthropological findings have suggested that the region was actually densely populated. Some 5 million people may have lived in the Amazon region in AD 1500, divided between dense coastal settlements, such as that at Marajó, and inland dwellers. By 1900 the population had fallen to 1 million and by the early 1980s it was less than 200,000.\n",
            "The first European to travel the length of the Amazon River was Francisco de Orellana in 1542. The BBC's Unnatural Histories presents evidence that Orellana, rather than exaggerating his claims as previously thought, was correct in his observations that a complex civilization was flourishing along the Amazon in the 1540s. It is believed that the civilization was later devastated by the spread of diseases from Europe, such as smallpox. Since the 1970s, numerous geoglyphs have been discovered on deforested land dating between AD 0–1250, furthering claims about Pre-Columbian civilizations. Ondemar Dias is accredited with first discovering the geoglyphs in 1977 and Alceu Ranzi with furthering their discovery after flying over Acre. The BBC's Unnatural Histories presented evidence that the Amazon rainforest, rather than being a pristine wilderness, has been shaped by man for at least 11,000 years through practices such as forest gardening and terra preta.\n",
            "Terra preta (black earth), which is distributed over large areas in the Amazon forest, is now widely accepted as a product of indigenous soil management. The development of this fertile soil allowed agriculture and silviculture in the previously hostile environment; meaning that large portions of the Amazon rainforest are probably the result of centuries of human management, rather than naturally occurring as has previously been supposed. In the region of the Xingu tribe, remains of some of these large settlements in the middle of the Amazon forest were found in 2003 by Michael Heckenberger and colleagues of the University of Florida. Among those were evidence of roads, bridges and large plazas.\n",
            "The region is home to about 2.5 million insect species, tens of thousands of plants, and some 2,000 birds and mammals. To date, at least 40,000 plant species, 2,200 fishes, 1,294 birds, 427 mammals, 428 amphibians, and 378 reptiles have been scientifically classified in the region. One in five of all the bird species in the world live in the rainforests of the Amazon, and one in five of the fish species live in Amazonian rivers and streams. Scientists have described between 96,660 and 128,843 invertebrate species in Brazil alone.\n",
            "The biodiversity of plant species is the highest on Earth with one 2001 study finding a quarter square kilometer (62 acres) of Ecuadorian rainforest supports more than 1,100 tree species. A study in 1999 found one square kilometer (247 acres) of Amazon rainforest can contain about 90,790 tonnes of living plants. The average plant biomass is estimated at 356 ± 47 tonnes per hectare. To date, an estimated 438,000 species of plants of economic and social interest have been registered in the region with many more remaining to be discovered or catalogued. The total number of tree species in the region is estimated at 16,000.\n",
            "The rainforest contains several species that can pose a hazard. Among the largest predatory creatures are the black caiman, jaguar, cougar, and anaconda. In the river, electric eels can produce an electric shock that can stun or kill, while piranha are known to bite and injure humans. Various species of poison dart frogs secrete lipophilic alkaloid toxins through their flesh. There are also numerous parasites and disease vectors. Vampire bats dwell in the rainforest and can spread the rabies virus. Malaria, yellow fever and Dengue fever can also be contracted in the Amazon region.\n",
            "Deforestation is the conversion of forested areas to non-forested areas. The main sources of deforestation in the Amazon are human settlement and development of the land. Prior to the early 1960s, access to the forest's interior was highly restricted, and the forest remained basically intact. Farms established during the 1960s were based on crop cultivation and the slash and burn method. However, the colonists were unable to manage their fields and the crops because of the loss of soil fertility and weed invasion. The soils in the Amazon are productive for just a short period of time, so farmers are constantly moving to new areas and clearing more land. These farming practices led to deforestation and caused extensive environmental damage. Deforestation is considerable, and areas cleared of forest are visible to the naked eye from outer space.\n",
            "Between 1991 and 2000, the total area of forest lost in the Amazon rose from 415,000 to 587,000 square kilometres (160,000 to 227,000 sq mi), with most of the lost forest becoming pasture for cattle. Seventy percent of formerly forested land in the Amazon, and 91% of land deforested since 1970, is used for livestock pasture. Currently, Brazil is the second-largest global producer of soybeans after the United States. New research however, conducted by Leydimere Oliveira et al., has shown that the more rainforest is logged in the Amazon, the less precipitation reaches the area and so the lower the yield per hectare becomes. So despite the popular perception, there has been no economical advantage for Brazil from logging rainforest zones and converting these to pastoral fields.\n",
            "The needs of soy farmers have been used to justify many of the controversial transportation projects that are currently developing in the Amazon. The first two highways successfully opened up the rainforest and led to increased settlement and deforestation. The mean annual deforestation rate from 2000 to 2005 (22,392 km2 or 8,646 sq mi per year) was 18% higher than in the previous five years (19,018 km2 or 7,343 sq mi per year). Although deforestation has declined significantly in the Brazilian Amazon between 2004 and 2014, there has been an increase to the present day.\n",
            "Environmentalists are concerned about loss of biodiversity that will result from destruction of the forest, and also about the release of the carbon contained within the vegetation, which could accelerate global warming. Amazonian evergreen forests account for about 10% of the world's terrestrial primary productivity and 10% of the carbon stores in ecosystems—of the order of 1.1 × 1011 metric tonnes of carbon. Amazonian forests are estimated to have accumulated 0.62 ± 0.37 tons of carbon per hectare per year between 1975 and 1996.\n",
            "One computer model of future climate change caused by greenhouse gas emissions shows that the Amazon rainforest could become unsustainable under conditions of severely reduced rainfall and increased temperatures, leading to an almost complete loss of rainforest cover in the basin by 2100. However, simulations of Amazon basin climate change across many different models are not consistent in their estimation of any rainfall response, ranging from weak increases to strong decreases. The result indicates that the rainforest could be threatened though the 21st century by climate change in addition to deforestation.\n",
            "As indigenous territories continue to be destroyed by deforestation and ecocide, such as in the Peruvian Amazon indigenous peoples' rainforest communities continue to disappear, while others, like the Urarina continue to struggle to fight for their cultural survival and the fate of their forested territories. Meanwhile, the relationship between non-human primates in the subsistence and symbolism of indigenous lowland South American peoples has gained increased attention, as have ethno-biology and community-based conservation efforts.\n",
            "The use of remote sensing for the conservation of the Amazon is also being used by the indigenous tribes of the basin to protect their tribal lands from commercial interests. Using handheld GPS devices and programs like Google Earth, members of the Trio Tribe, who live in the rainforests of southern Suriname, map out their ancestral lands to help strengthen their territorial claims. Currently, most tribes in the Amazon do not have clearly defined boundaries, making it easier for commercial ventures to target their territories.\n",
            "To accurately map the Amazon's biomass and subsequent carbon related emissions, the classification of tree growth stages within different parts of the forest is crucial. In 2006 Tatiana Kuplich organized the trees of the Amazon into four categories: (1) mature forest, (2) regenerating forest [less than three years], (3) regenerating forest [between three and five years of regrowth], and (4) regenerating forest [eleven to eighteen years of continued development]. The researcher used a combination of Synthetic aperture radar (SAR) and Thematic Mapper (TM) to accurately place the different portions of the Amazon into one of the four classifications.\n",
            "In 2005, parts of the Amazon basin experienced the worst drought in one hundred years, and there were indications that 2006 could have been a second successive year of drought. A July 23, 2006 article in the UK newspaper The Independent reported Woods Hole Research Center results showing that the forest in its present form could survive only three years of drought. Scientists at the Brazilian National Institute of Amazonian Research argue in the article that this drought response, coupled with the effects of deforestation on regional climate, are pushing the rainforest towards a \"tipping point\" where it would irreversibly start to die. It concludes that the forest is on the brink of being turned into savanna or desert, with catastrophic consequences for the world's climate.\n",
            "In 2010 the Amazon rainforest experienced another severe drought, in some ways more extreme than the 2005 drought. The affected region was approximate 1,160,000 square miles (3,000,000 km2) of rainforest, compared to 734,000 square miles (1,900,000 km2) in 2005. The 2010 drought had three epicenters where vegetation died off, whereas in 2005 the drought was focused on the southwestern part. The findings were published in the journal Science. In a typical year the Amazon absorbs 1.5 gigatons of carbon dioxide; during 2005 instead 5 gigatons were released and in 2010 8 gigatons were released.\n",
            "\n",
            "There are three major types of rock: igneous, sedimentary, and metamorphic. The rock cycle is an important concept in geology which illustrates the relationships between these three types of rock, and magma. When a rock crystallizes from melt (magma and/or lava), it is an igneous rock. This rock can be weathered and eroded, and then redeposited and lithified into a sedimentary rock, or be turned into a metamorphic rock due to heat and pressure that change the mineral content of the rock which gives it a characteristic fabric. The sedimentary rock can then be subsequently turned into a metamorphic rock due to heat and pressure and is then weathered, eroded, deposited, and lithified, ultimately becoming a sedimentary rock. Sedimentary rock may also be re-eroded and redeposited, and metamorphic rock may also undergo additional metamorphism. All three types of rocks may be re-melted; when this happens, a new magma is formed, from which an igneous rock may once again crystallize.\n",
            "In the 1960s, a series of discoveries, the most important of which was seafloor spreading, showed that the Earth's lithosphere, which includes the crust and rigid uppermost portion of the upper mantle, is separated into a number of tectonic plates that move across the plastically deforming, solid, upper mantle, which is called the asthenosphere. There is an intimate coupling between the movement of the plates on the surface and the convection of the mantle: oceanic plate motions and mantle convection currents always move in the same direction, because the oceanic lithosphere is the rigid upper thermal boundary layer of the convecting mantle. This coupling between rigid plates moving on the surface of the Earth and the convecting mantle is called plate tectonics.\n",
            "The development of plate tectonics provided a physical basis for many observations of the solid Earth. Long linear regions of geologic features could be explained as plate boundaries. Mid-ocean ridges, high regions on the seafloor where hydrothermal vents and volcanoes exist, were explained as divergent boundaries, where two plates move apart. Arcs of volcanoes and earthquakes were explained as convergent boundaries, where one plate subducts under another. Transform boundaries, such as the San Andreas fault system, resulted in widespread powerful earthquakes. Plate tectonics also provided a mechanism for Alfred Wegener's theory of continental drift, in which the continents move across the surface of the Earth over geologic time. They also provided a driving force for crustal deformation, and a new setting for the observations of structural geology. The power of the theory of plate tectonics lies in its ability to combine all of these observations into a single theory of how the lithosphere moves over the convecting mantle.\n",
            "Seismologists can use the arrival times of seismic waves in reverse to image the interior of the Earth. Early advances in this field showed the existence of a liquid outer core (where shear waves were not able to propagate) and a dense solid inner core. These advances led to the development of a layered model of the Earth, with a crust and lithosphere on top, the mantle below (separated within itself by seismic discontinuities at 410 and 660 kilometers), and the outer core and inner core below that. More recently, seismologists have been able to create detailed images of wave speeds inside the earth in the same way a doctor images a body in a CT scan. These images have led to a much more detailed view of the interior of the Earth, and have replaced the simplified layered model with a much more dynamic model.\n",
            "The following four timelines show the geologic time scale. The first shows the entire time from the formation of the Earth to the present, but this compresses the most recent eon. Therefore, the second scale shows the most recent eon with an expanded scale. The second scale compresses the most recent era, so the most recent era is expanded in the third scale. Since the Quaternary is a very short period with short epochs, it is further expanded in the fourth scale. The second, third, and fourth timelines are therefore each subsections of their preceding timeline as indicated by asterisks. The Holocene (the latest epoch) is too small to be shown clearly on the third timeline on the right, another reason for expanding the fourth scale. The Pleistocene (P) epoch. Q stands for the Quaternary period.\n",
            "The principle of cross-cutting relationships pertains to the formation of faults and the age of the sequences through which they cut. Faults are younger than the rocks they cut; accordingly, if a fault is found that penetrates some formations but not those on top of it, then the formations that were cut are older than the fault, and the ones that are not cut must be younger than the fault. Finding the key bed in these situations may help determine whether the fault is a normal fault or a thrust fault.\n",
            "The principle of inclusions and components states that, with sedimentary rocks, if inclusions (or clasts) are found in a formation, then the inclusions must be older than the formation that contains them. For example, in sedimentary rocks, it is common for gravel from an older formation to be ripped up and included in a newer layer. A similar situation with igneous rocks occurs when xenoliths are found. These foreign bodies are picked up as magma or lava flows, and are incorporated, later to cool in the matrix. As a result, xenoliths are older than the rock which contains them.\n",
            "The principle of faunal succession is based on the appearance of fossils in sedimentary rocks. As organisms exist at the same time period throughout the world, their presence or (sometimes) absence may be used to provide a relative age of the formations in which they are found. Based on principles laid out by William Smith almost a hundred years before the publication of Charles Darwin's theory of evolution, the principles of succession were developed independently of evolutionary thought. The principle becomes quite complex, however, given the uncertainties of fossilization, the localization of fossil types due to lateral changes in habitat (facies change in sedimentary strata), and that not all fossils may be found globally at the same time.\n",
            "At the beginning of the 20th century, important advancement in geological science was facilitated by the ability to obtain accurate absolute dates to geologic events using radioactive isotopes and other methods. This changed the understanding of geologic time. Previously, geologists could only use fossils and stratigraphic correlation to date sections of rock relative to one another. With isotopic dates it became possible to assign absolute ages to rock units, and these absolute dates could be applied to fossil sequences in which there was datable material, converting the old relative ages into new absolute ages.\n",
            "For many geologic applications, isotope ratios of radioactive elements are measured in minerals that give the amount of time that has passed since a rock passed through its particular closure temperature, the point at which different radiometric isotopes stop diffusing into and out of the crystal lattice. These are used in geochronologic and thermochronologic studies. Common methods include uranium-lead dating, potassium-argon dating, argon-argon dating and uranium-thorium dating. These methods are used for a variety of applications. Dating of lava and volcanic ash layers found within a stratigraphic sequence can provide absolute age data for sedimentary rock units which do not contain radioactive isotopes and calibrate relative dating techniques. These methods can also be used to determine ages of pluton emplacement. Thermochemical techniques can be used to determine temperature profiles within the crust, the uplift of mountain ranges, and paleotopography.\n",
            "When rock units are placed under horizontal compression, they shorten and become thicker. Because rock units, other than muds, do not significantly change in volume, this is accomplished in two primary ways: through faulting and folding. In the shallow crust, where brittle deformation can occur, thrust faults form, which cause deeper rock to move on top of shallower rock. Because deeper rock is often older, as noted by the principle of superposition, this can result in older rocks moving on top of younger ones. Movement along faults can result in folding, either because the faults are not planar or because rock layers are dragged along, forming drag folds as slip occurs along the fault. Deeper in the Earth, rocks behave plastically, and fold instead of faulting. These folds can either be those where the material in the center of the fold buckles upwards, creating \"antiforms\", or where it buckles downwards, creating \"synforms\". If the tops of the rock units within the folds remain pointing upwards, they are called anticlines and synclines, respectively. If some of the units in the fold are facing downward, the structure is called an overturned anticline or syncline, and if all of the rock units are overturned or the correct up-direction is unknown, they are simply called by the most general terms, antiforms and synforms.\n",
            "Extension causes the rock units as a whole to become longer and thinner. This is primarily accomplished through normal faulting and through the ductile stretching and thinning. Normal faults drop rock units that are higher below those that are lower. This typically results in younger units being placed below older units. Stretching of units can result in their thinning; in fact, there is a location within the Maria Fold and Thrust Belt in which the entire sedimentary sequence of the Grand Canyon can be seen over a length of less than a meter. Rocks at the depth to be ductilely stretched are often also metamorphosed. These stretched rocks can also pinch into lenses, known as boudins, after the French word for \"sausage\", because of their visual similarity.\n",
            "The addition of new rock units, both depositionally and intrusively, often occurs during deformation. Faulting and other deformational processes result in the creation of topographic gradients, causing material on the rock unit that is increasing in elevation to be eroded by hillslopes and channels. These sediments are deposited on the rock unit that is going down. Continual motion along the fault maintains the topographic gradient in spite of the movement of sediment, and continues to create accommodation space for the material to deposit. Deformational events are often also associated with volcanism and igneous activity. Volcanic ashes and lavas accumulate on the surface, and igneous intrusions enter from below. Dikes, long, planar igneous intrusions, enter along cracks, and therefore often form in large numbers in areas that are being actively deformed. This can result in the emplacement of dike swarms, such as those that are observable across the Canadian shield, or rings of dikes around the lava tube of a volcano.\n",
            "All of these processes do not necessarily occur in a single environment, and do not necessarily occur in a single order. The Hawaiian Islands, for example, consist almost entirely of layered basaltic lava flows. The sedimentary sequences of the mid-continental United States and the Grand Canyon in the southwestern United States contain almost-undeformed stacks of sedimentary rocks that have remained in place since Cambrian time. Other areas are much more geologically complex. In the southwestern United States, sedimentary, volcanic, and intrusive rocks have been metamorphosed, faulted, foliated, and folded. Even older rocks, such as the Acasta gneiss of the Slave craton in northwestern Canada, the oldest known rock in the world have been metamorphosed to the point where their origin is undiscernable without laboratory analysis. In addition, these processes can occur in stages. In many places, the Grand Canyon in the southwestern United States being a very visible example, the lower rock units were metamorphosed and deformed, and then deformation ended and the upper, undeformed units were deposited. Although any amount of rock emplacement and rock deformation can occur, and they can occur any number of times, these concepts provide a guide to understanding the geological history of an area.\n",
            "Geologists use a number of field, laboratory, and numerical modeling methods to decipher Earth history and understand the processes that occur on and inside the Earth. In typical geological investigations, geologists use primary information related to petrology (the study of rocks), stratigraphy (the study of sedimentary layers), and structural geology (the study of positions of rock units and their deformation). In many cases, geologists also study modern soils, rivers, landscapes, and glaciers; investigate past and current life and biogeochemical pathways, and use geophysical methods to investigate the subsurface.\n",
            "In addition to identifying rocks in the field, petrologists identify rock samples in the laboratory. Two of the primary methods for identifying rocks in the laboratory are through optical microscopy and by using an electron microprobe. In an optical mineralogy analysis, thin sections of rock samples are analyzed through a petrographic microscope, where the minerals can be identified through their different properties in plane-polarized and cross-polarized light, including their birefringence, pleochroism, twinning, and interference properties with a conoscopic lens. In the electron microprobe, individual locations are analyzed for their exact chemical compositions and variation in composition within individual crystals. Stable and radioactive isotope studies provide insight into the geochemical evolution of rock units.\n",
            "Petrologists can also use fluid inclusion data and perform high temperature and pressure physical experiments to understand the temperatures and pressures at which different mineral phases appear, and how they change through igneous and metamorphic processes. This research can be extrapolated to the field to understand metamorphic processes and the conditions of crystallization of igneous rocks. This work can also help to explain processes that occur within the Earth, such as subduction and magma chamber evolution.\n",
            "Structural geologists use microscopic analysis of oriented thin sections of geologic samples to observe the fabric within the rocks which gives information about strain within the crystalline structure of the rocks. They also plot and combine measurements of geological structures in order to better understand the orientations of faults and folds in order to reconstruct the history of rock deformation in the area. In addition, they perform analog and numerical experiments of rock deformation in large and small settings.\n",
            "Among the most well-known experiments in structural geology are those involving orogenic wedges, which are zones in which mountains are built along convergent tectonic plate boundaries. In the analog versions of these experiments, horizontal layers of sand are pulled along a lower surface into a back stop, which results in realistic-looking patterns of faulting and the growth of a critically tapered (all angles remain the same) orogenic wedge. Numerical models work in the same way as these analog models, though they are often more sophisticated and can include patterns of erosion and uplift in the mountain belt. This helps to show the relationship between erosion and the shape of the mountain range. These studies can also give useful information about pathways for metamorphism through pressure, temperature, space, and time.\n",
            "In the laboratory, stratigraphers analyze samples of stratigraphic sections that can be returned from the field, such as those from drill cores. Stratigraphers also analyze data from geophysical surveys that show the locations of stratigraphic units in the subsurface. Geophysical data and well logs can be combined to produce a better view of the subsurface, and stratigraphers often use computer programs to do this in three dimensions. Stratigraphers can then use these data to reconstruct ancient processes occurring on the surface of the Earth, interpret past environments, and locate areas for water, coal, and hydrocarbon extraction.\n",
            "In the laboratory, biostratigraphers analyze rock samples from outcrop and drill cores for the fossils found in them. These fossils help scientists to date the core and to understand the depositional environment in which the rock units formed. Geochronologists precisely date rocks within the stratigraphic section in order to provide better absolute bounds on the timing and rates of deposition. Magnetic stratigraphers look for signs of magnetic reversals in igneous rock units within the drill cores. Other scientists perform stable isotope studies on the rocks to gain information about past climate.\n",
            "Some modern scholars, such as Fielding H. Garrison, are of the opinion that the origin of the science of geology can be traced to Persia after the Muslim conquests had come to an end. Abu al-Rayhan al-Biruni (973–1048 CE) was one of the earliest Persian geologists, whose works included the earliest writings on the geology of India, hypothesizing that the Indian subcontinent was once a sea. Drawing from Greek and Indian scientific literature that were not destroyed by the Muslim conquests, the Persian scholar Ibn Sina (Avicenna, 981–1037) proposed detailed explanations for the formation of mountains, the origin of earthquakes, and other topics central to modern geology, which provided an essential foundation for the later development of the science. In China, the polymath Shen Kuo (1031–1095) formulated a hypothesis for the process of land formation: based on his observation of fossil animal shells in a geological stratum in a mountain hundreds of miles from the ocean, he inferred that the land was formed by erosion of the mountains and by deposition of silt.\n",
            "James Hutton is often viewed as the first modern geologist. In 1785 he presented a paper entitled Theory of the Earth to the Royal Society of Edinburgh. In his paper, he explained his theory that the Earth must be much older than had previously been supposed in order to allow enough time for mountains to be eroded and for sediments to form new rocks at the bottom of the sea, which in turn were raised up to become dry land. Hutton published a two-volume version of his ideas in 1795 (Vol. 1, Vol. 2).\n",
            "The first geological map of the U.S. was produced in 1809 by William Maclure. In 1807, Maclure commenced the self-imposed task of making a geological survey of the United States. Almost every state in the Union was traversed and mapped by him, the Allegheny Mountains being crossed and recrossed some 50 times. The results of his unaided labours were submitted to the American Philosophical Society in a memoir entitled Observations on the Geology of the United States explanatory of a Geological Map, and published in the Society's Transactions, together with the nation's first geological map. This antedates William Smith's geological map of England by six years, although it was constructed using a different classification of rocks.\n",
            "Sir Charles Lyell first published his famous book, Principles of Geology, in 1830. This book, which influenced the thought of Charles Darwin, successfully promoted the doctrine of uniformitarianism. This theory states that slow geological processes have occurred throughout the Earth's history and are still occurring today. In contrast, catastrophism is the theory that Earth's features formed in single, catastrophic events and remained unchanged thereafter. Though Hutton believed in uniformitarianism, the idea was not widely accepted at the time.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iHRg9fd2jB2"
      },
      "source": [
        "# **Quarc Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDxsCuk1Robb"
      },
      "source": [
        "class Quarc:\n",
        "  \n",
        "  def __init__(self, corpus):\n",
        "    self.corpus = corpus\n",
        "\n",
        "    # Contains all the stop words...\n",
        "    self.raw_sentence_list = [self.text_preprocess(sent) for sent in sent_tokenize(self.corpus)]\n",
        "\n",
        "    self.preprocessed_sentence_list = [self.remove_stop_words(sent, sentence_stop_words) for sent in self.raw_sentence_list]\n",
        "\n",
        "    self.sentence_score = list(np.zeros(len(self.raw_sentence_list), dtype=int))\n",
        "\n",
        "    # Four Possible Point Values\n",
        "    self.clue = 3\n",
        "    self.good_clue = 4\n",
        "    self.confident = 6\n",
        "    self.slam_dunk = 20\n",
        "\n",
        "  @staticmethod\n",
        "  def text_preprocess(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub('[^a-z0-9_]', ' ', sentence)\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "    return sentence\n",
        "  \n",
        "  @staticmethod\n",
        "  def tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('[^a-z0-9_]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return [wnl.lemmatize(word) for word in word_tokenize(text) if not word in stop_words]\n",
        "\n",
        "  @staticmethod\n",
        "  def remove_stop_words(sentence, stop_word_list):\n",
        "    return \" \".join([porter_stemmer.stem(wnl.lemmatize(word)) for word in word_tokenize(sentence) if not word in stop_word_list])\n",
        "\n",
        "  @staticmethod\n",
        "  def contains_name(text):\n",
        "    # text ---> stopwords and punctuations are removed...\n",
        "    doc = nlp(text)\n",
        "    flag = False\n",
        "    for token in doc:\n",
        "        if token.pos_ == 'PROPN':\n",
        "            flag = True\n",
        "            break\n",
        "    if flag:\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ == 'PERSON':\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "  @staticmethod\n",
        "  def contains_keyword(word_list, text):\n",
        "    # text ---> stopwords and punctuations are removed...\n",
        "    if any(word in text for word in word_list):\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  def word_match(self, question, sentence):   \n",
        "    # question, sentence ---> stop_words should be removed    \n",
        "    score_word_match = 0\n",
        "    question_ = question.split()\n",
        "    sentence_ = sentence.split()\n",
        "\n",
        "    # intersection of question and senctence\n",
        "    intersection = list(set(question_).intersection(sentence_))\n",
        "    doc = nlp(' '.join(intersection))\n",
        "    for token in doc:\n",
        "      if token.pos_ == 'VERB':\n",
        "        score_word_match += self.confident # self.confident = 6 points\n",
        "      else:\n",
        "        score_word_match += self.clue # self.clue = 3 points\n",
        "    return score_word_match\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ityk-IlQI9U2"
      },
      "source": [
        "# **Who model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9tTr2VhkNSq"
      },
      "source": [
        "class Who(Quarc):\n",
        "\n",
        "  def __init__(self, corpus):\n",
        "    super().__init__(corpus)\n",
        "\n",
        "  \n",
        "  def get_answer(self, question):\n",
        "    # question ---> (raw question) contains stopwords, punctuations...\n",
        "    \n",
        "    # Preprocess the question and remove the stopwords\n",
        "    # \"Where are you going, this early in the morning?\" ---> \"go early morning\"\n",
        "    question = self.text_preprocess(question)\n",
        "    question = self.remove_stop_words(question, question_stop_words)\n",
        "\n",
        "    # Now, find the score of all the sentences...\n",
        "\n",
        "    for i, sent in enumerate(self.preprocessed_sentence_list):\n",
        "      self.sentence_score[i] = self.get_score(question, sent)\n",
        "    \n",
        "    max_index = self.sentence_score.index(max(self.sentence_score))\n",
        "\n",
        "    if(self.sentence_score[max_index] == 0):\n",
        "      print(\"Alert: max_score is zero!!\")\n",
        "\n",
        "    # Now, depending upon the max_score value, return the appropriate sentence\n",
        "    return self.raw_sentence_list[max_index]\n",
        "\n",
        "  def get_score(self, question, sentence):\n",
        "    score_1 = self.who_rule_1(question, sentence)\n",
        "    score_2 = self.who_rule_2(question, sentence)\n",
        "    score_3 = self.who_rule_3(question, sentence)\n",
        "    score_4 = self.who_rule_4(sentence)\n",
        "\n",
        "    return score_1+score_2+score_3+score_4\n",
        "\n",
        "\n",
        "  def who_rule_1(self, question, sentence):\n",
        "    return self.word_match(question, sentence)\n",
        "\n",
        "  def who_rule_2(self, question, sentence):\n",
        "    if not self.contains_name(question) and self.contains_name(sentence):\n",
        "      return self.confident # self.confident = 6 points\n",
        "    return 0\n",
        "\n",
        "  def who_rule_3(self, question, sentence):\n",
        "    if not self.contains_name(question) and self.contains_keyword([\"name\"], sentence):\n",
        "      return self.good_clue # self.good_clue = 4 points\n",
        "    return 0\n",
        "\n",
        "  def who_rule_4(self, sentence):\n",
        "    if self.contains_name(sentence):\n",
        "      return self.good_clue # self.good_clue = 4 points\n",
        "    return 0\n",
        "\n",
        "  def extract_answer(self, text, question=None):\n",
        "    #who type question ---> ner: PERSON\n",
        "    doc = nlp(text)\n",
        "    flag = False\n",
        "    for token in doc:\n",
        "        if token.pos_ == 'PROPN':\n",
        "            flag = True\n",
        "            break\n",
        "    if flag:\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ == 'PERSON':\n",
        "                return str(ent)\n",
        "    return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKLlxwetKOFg"
      },
      "source": [
        "# **When Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOWjZNzWKMu_"
      },
      "source": [
        "class When(Quarc):\n",
        "\n",
        "  def __init__(self, corpus):\n",
        "    super().__init__(corpus)\n",
        "\n",
        "  \n",
        "  def get_answer(self, question):\n",
        "    # Preprocess the question and remove the stopwords\n",
        "    question = self.text_preprocess(question)\n",
        "    question = self.remove_stop_words(question, question_stop_words)\n",
        "\n",
        "    # Now, find the score of all the sentences...\n",
        "    for i, sent in enumerate(self.preprocessed_sentence_list):\n",
        "      self.sentence_score[i] = self.get_score(question, sent)\n",
        "    \n",
        "    max_index = self.sentence_score.index(max(self.sentence_score))\n",
        "\n",
        "    if(self.sentence_score[max_index] == 0):\n",
        "      print(\"Alert: max_score is zero!!\")\n",
        "\n",
        "    # Now, depending upon the max_score value, return the appropriate sentence\n",
        "    return self.raw_sentence_list[max_index]\n",
        "\n",
        "  def get_score(self, question, sentence):\n",
        "    score_1 = self.when_rule_1(question, sentence)\n",
        "    score_2 = self.when_rule_2(question, sentence)\n",
        "    score_3 = self.when_rule_3(question, sentence)\n",
        "\n",
        "    return score_1+score_2+score_3\n",
        "\n",
        "  @staticmethod\n",
        "  def contains_time(sentence):\n",
        "    # question ---> stop_words and punctuations are removed...\n",
        "    doc = nlp(sentence)\n",
        "    for ent in doc.ents:\n",
        "      if ent.label_ == \"DATE\":\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "  def when_rule_1(self, question, sentence):\n",
        "    if self.contains_time(sentence):\n",
        "      return self.word_match(question, sentence) + self.good_clue # self.good_clue = 4 points\n",
        "    return 0\n",
        "\n",
        "  def when_rule_2(self, question, sentence):\n",
        "      if self.contains_keyword([\"the last\"], question) and self.contains_keyword([\"first\", \"last\", \"since\", \"ago\"], sentence):\n",
        "        return self.slam_dunk # self.slam_dunk = 20\n",
        "      return 0\n",
        "\n",
        "  def when_rule_3(self, question, sentence):\n",
        "      if self.contains_keyword([\"start\", \"begin\"], question) and self.contains_keyword([\"start\", \"begin\", \"since\", \"year\", \"during\"], sentence):\n",
        "        return self.slam_dunk # self.slam_dunk = 20\n",
        "      return 0\n",
        "  \n",
        "  def extract_answer(self, text, question=None):\n",
        "\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "      if ent.label_ == 'DATE' or ent.label_ == 'TIME':\n",
        "        return str(ent)\n",
        "        \n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3poxFfBJ_G1"
      },
      "source": [
        "# **Where Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfrJlzKLJ-BJ"
      },
      "source": [
        "class Where(Quarc):\n",
        "\n",
        "  def __init__(self, corpus):\n",
        "    super().__init__(corpus)\n",
        "\n",
        "  def get_answer(self, question):\n",
        "    # Preprocess the question and remove the stopwords\n",
        "    question = self.text_preprocess(question)\n",
        "    question = self.remove_stop_words(question, question_stop_words)\n",
        "\n",
        "    # Now, find the score of all the sentences...\n",
        "    for i, sent in enumerate(self.preprocessed_sentence_list):\n",
        "      self.sentence_score[i] = self.get_score(question, sent)\n",
        "    \n",
        "    max_index = self.sentence_score.index(max(self.sentence_score))\n",
        "\n",
        "    if(self.sentence_score[max_index] == 0):\n",
        "      print(\"Alert: max_score is zero!!\")\n",
        "\n",
        "    # Now, depending upon the max_score value, return the appropriate sentence\n",
        "    return self.raw_sentence_list[max_index]\n",
        "\n",
        "  def get_score(self, question, sentence):\n",
        "    score_1 = self.where_rule_1(question, sentence)\n",
        "    score_2 = self.where_rule_2(sentence)\n",
        "    score_3 = self.where_rule_3(sentence)\n",
        "\n",
        "    return score_1+score_2+score_3\n",
        "\n",
        "  @staticmethod\n",
        "  def contains_loc(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    for ent in doc.ents:\n",
        "      if ent.label_ == \"GPE\":\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "  def where_rule_1(self, question, sentence):\n",
        "    return self.word_match(question, sentence)\n",
        "\n",
        "  def where_rule_2(self, sentence):\n",
        "    prep_list = [\"in\", \"at\", \"on\", \"above\", \"below\", \"near\", \"inside\", \"next to\", \"in between\", \"behind\",\n",
        "                  \"under\", \"inside\", \"beneath\", \"far from\", \"across\"]\n",
        "    if self.contains_keyword(prep_list, sentence):\n",
        "      return self.good_clue # self.good_clue = 4\n",
        "    return 0\n",
        "\n",
        "  def where_rule_3(self, sentence):\n",
        "    if self.contains_loc(sentence):\n",
        "      return self.confident # self.confident = 6\n",
        "    return 0\n",
        "\n",
        "  def extract_answer(self, text, question=None):\n",
        "    #who type question ---> ner: PERSON\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "      if ent.label_ in ['LOC', 'FAC', 'GPE', 'ORG']:\n",
        "        return str(ent)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT9GhmWqJ1jp"
      },
      "source": [
        "# **Why Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkBkQpkcJ0UE"
      },
      "source": [
        "class Why(Quarc):\n",
        "\n",
        "  def __init__(self, corpus):\n",
        "    super().__init__(corpus)\n",
        "    # Best Sentence for Why Rule\n",
        "    self.best_sentences = []\n",
        "\n",
        "  def get_answer(self, question):\n",
        "    # Preprocess the question and remove the stopwords\n",
        "    question = self.text_preprocess(question)\n",
        "    question = self.remove_stop_words(question, question_stop_words)\n",
        "\n",
        "    # It will fetch all the best sentences which match the given question...\n",
        "    self.find_best_sentences(question)\n",
        "\n",
        "    # Now, find the score of all the sentences...\n",
        "    for i, sent in enumerate(self.preprocessed_sentence_list):\n",
        "      self.sentence_score[i] = self.get_score(question, sent)\n",
        "    \n",
        "    max_index = self.sentence_score.index(max(self.sentence_score))\n",
        "\n",
        "    if(self.sentence_score[max_index] == 0):\n",
        "      print(\"Alert: max_score is zero!!\")\n",
        "\n",
        "    # Now, depending upon the max_score value, return the appropriate sentence\n",
        "    return self.raw_sentence_list[max_index]\n",
        "\n",
        "  def get_score(self, question, sentence):\n",
        "    score_1 = self.why_rule_1(sentence)\n",
        "    score_2 = self.why_rule_2(sentence)\n",
        "    score_3 = self.why_rule_3(sentence)\n",
        "    score_4 = self.why_rule_4(sentence)\n",
        "    score_5 = self.why_rule_5(sentence)\n",
        "\n",
        "\n",
        "    return score_1+score_2+score_3+score_4+score_5\n",
        "\n",
        "  def find_best_sentences(self, question):\n",
        "    for sentence in self.preprocessed_sentence_list:\n",
        "      if self.word_match(question, sentence) > 0:\n",
        "        self.best_sentences.append(sentence)\n",
        "\n",
        "  def why_rule_1(self, sentence):\n",
        "    if sentence in self.best_sentences:\n",
        "      return self.clue # self.clue = 3\n",
        "    return 0\n",
        "\n",
        "  def why_rule_2(self, sentence):\n",
        "    cur_ind = self.preprocessed_sentence_list.index(sentence)\n",
        "    sent_len = len(self.preprocessed_sentence_list)\n",
        "    if cur_ind < sent_len-1 and self.preprocessed_sentence_list[cur_ind+1] in self.best_sentences:\n",
        "      return self.clue # self.clue = 3\n",
        "    return 0\n",
        "\n",
        "  def why_rule_3(self, sentence):\n",
        "    cur_ind = self.preprocessed_sentence_list.index(sentence)\n",
        "    if cur_ind > 0 and self.preprocessed_sentence_list[cur_ind-1] in self.best_sentences:\n",
        "      return self.good_clue # self.good_clue = 4\n",
        "    return 0\n",
        "\n",
        "  def why_rule_4(self, sentence):\n",
        "    if self.contains_keyword([\"want\"], sentence):\n",
        "      return self.good_clue # self.good_clue = 4\n",
        "    return 0\n",
        "\n",
        "  def why_rule_5(self, sentence):\n",
        "    if self.contains_keyword([\"so\", \"because\"], sentence):\n",
        "      return self.good_clue # self.good_clue = 4\n",
        "    return 0\n",
        "\n",
        "  def extract_answer(self, text, question=None):\n",
        "\n",
        "    #find the intersection between text and question\n",
        "    ans = \"\"\n",
        "    question = Quarc.text_preprocess(question)\n",
        "    for token in text.split():\n",
        "      if not token in question.split() and not token in stop_words:\n",
        "        ans += token + \" \"    \n",
        "    return ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYrjMyzvJjpZ"
      },
      "source": [
        "# **DateLine Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIf8MBDLJihC"
      },
      "source": [
        "class DateLine(Quarc):\n",
        "  def dateline_rule_1(self, question):\n",
        "    if contains_keyword([\"happen\"], question):\n",
        "      return self.good_clue # self.good_clue = 4\n",
        "    return 0\n",
        "\n",
        "  def dateline_rule_2(self, question):\n",
        "    if contains_keyword([\"take\"], question) and contains_keyword([\"place\"], question):\n",
        "      return self.good_clue # self.good_clue = 4\n",
        "    return 0\n",
        "\n",
        "  def dateline_rule_3(self, question):\n",
        "    if contains_keyword([\"this\"], question):\n",
        "      return self.slam_dunk # self.slam_dunk = 20\n",
        "    return 0\n",
        "\n",
        "  def dateline_rule_4(self, question):\n",
        "    if contains_keyword([\"story\"], question):\n",
        "      return self.slam_dunk # self.slam_dunk = 20\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HzvLF5eJZEG"
      },
      "source": [
        "# **What Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixMxnJpZJWK9"
      },
      "source": [
        "class What(Quarc):\n",
        "\n",
        "  def __init__(self, corpus):\n",
        "    super().__init__(corpus)\n",
        "\n",
        "  \n",
        "  def get_answer(self, question):\n",
        "    # Preprocess the question and remove the stopwords\n",
        "    question = self.text_preprocess(question)\n",
        "    question = self.remove_stop_words(question, question_stop_words)\n",
        "\n",
        "    # Now, find the score of all the sentences...\n",
        "    for i, sent in enumerate(self.preprocessed_sentence_list):\n",
        "      self.sentence_score[i] = self.get_score(question, sent)\n",
        "    \n",
        "    max_index = self.sentence_score.index(max(self.sentence_score))\n",
        "\n",
        "    if(self.sentence_score[max_index] == 0):\n",
        "      print(\"Alert: max_score is zero!!\")\n",
        "\n",
        "    # Now, depending upon the max_score value, return the appropriate sentence\n",
        "    return self.raw_sentence_list[max_index]\n",
        "\n",
        "  def get_score(self, question, sentence):\n",
        "    score_1 = self.what_rule_1(question, sentence)\n",
        "    score_2 = self.what_rule_2(question, sentence)\n",
        "    score_3 = self.what_rule_3(question, sentence)\n",
        "    score_4 = self.what_rule_4(question, sentence)\n",
        "\n",
        "    return score_1+score_2+score_3+score_4\n",
        "\n",
        "  def what_rule_1(self, question, sentence):\n",
        "    return self.word_match(question, sentence)\n",
        "\n",
        "  def what_rule_2(self, question, sentence):\n",
        "    month_list = [\"january\", \"jan\", \"february\", \"feb\", \"march\", \"mar\", \"april\", \"apr\", \"may\", \"june\", \"jun\", \"july\", \"jul\", \n",
        "                    \"august\", \"aug\", \"september\", \"sept\", \"sep\", \"october\", \"oct\", \"november\", \"nov\", \"december\", \"dec\"]\n",
        "    day_list = [\"today\", \"yesterday\", \"yesterday\", \"last night\"]\n",
        "\n",
        "    if self.contains_keyword(month_list, question) and self.contains_keyword(day_list, sentence):\n",
        "      return self.clue # self.clue = 3 points\n",
        "    return 0\n",
        "\n",
        "  def what_rule_3(self, question, sentence):\n",
        "    if self.contains_keyword([\"kind\"], question) and self.contains_keyword([\"call\", \"from\"], sentence):\n",
        "      return self.good_clue # self.good_clue = 4 points\n",
        "    return 0\n",
        "\n",
        "  def what_rule_4(self, question, sentence):\n",
        "    if self.contains_keyword([\"name\"], question) and self.contains_keyword([\"name\", \"call\", \"known\"], sentence):\n",
        "      return self.clue # self.clue = 3 points\n",
        "    return 0\n",
        "\n",
        "  @staticmethod\n",
        "  def get_head_pp(question):\n",
        "\n",
        "      # name of the house? ---> \"house\"\n",
        "      # name of the creek? ---> \"creek\"\n",
        "\n",
        "      # \"What was the name of the window?\"\n",
        "    \n",
        "      # \"name <ADJ> <PROPN> <NOUN>\"\n",
        "\n",
        "    question_list = question.split()\n",
        "    name_index = question_list.index('name')\n",
        "\n",
        "    try:\n",
        "      if(nlp(question_list[name_index+1])[0].pos_ is \"ADJ\"):\n",
        "        return question_list[name_index+2]\n",
        "\n",
        "      elif (nlp(question_list[name_index+1])[0].pos_ is \"PROPN\" or \"NOUN\"):\n",
        "        return question_list[name_index+1]\n",
        "\n",
        "      else:\n",
        "        return None\n",
        "\n",
        "    except:\n",
        "      return None\n",
        "\n",
        "\n",
        "  def what_rule_5(self, question, sentence):\n",
        "    if self.contains_name(sentence) and self.contains_keyword([\"name\"], question):\n",
        "      head_pp = self.get_head_pp(question)\n",
        "\n",
        "      if(head_pp is None):\n",
        "        return 0\n",
        "\n",
        "      if self.contains_keyword([head_pp], sentence):\n",
        "        return self.slam_dunk # self.slam_dun = 20 points\n",
        "    return 0\n",
        "\n",
        "  def extract_answer(self, text, question=None):\n",
        "\n",
        "    #find the intersection between text and question\n",
        "    ans = \"\"\n",
        "    question = Quarc.text_preprocess(question)\n",
        "    for token in text.split():\n",
        "      if not token in question.split() and not token in stop_words:\n",
        "        ans += token + \" \"\n",
        "    \n",
        "    return ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYSd9pE5KgUa"
      },
      "source": [
        "# **Question Answering**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVHYSBZoIgI7"
      },
      "source": [
        "# Build a dictionary {\"type_of_wh_question\": \"Class\"}\n",
        "wh_dict = {\"who\": Who, \"what\": What, \"when\": When, \"where\": Where, \"why\": Why}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCtv4591ccTS"
      },
      "source": [
        "def get_q_type(question):\n",
        "\n",
        "  question = question.lower()\n",
        "\n",
        "  if \"what\" in question:\n",
        "    return \"what\"\n",
        "  elif \"who\" in question:\n",
        "    return \"who\"\n",
        "  elif \"when\" in question:\n",
        "    return \"when\"\n",
        "  elif \"where\" in question:\n",
        "    return \"where\"\n",
        "  elif \"why\" in question:\n",
        "    return \"why\"\n",
        "  \n",
        "  else:\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQcWS0JmzwHE",
        "outputId": "85f2e09e-6609-42df-9c5f-727bd98f261f"
      },
      "source": [
        "while(True):\n",
        "  question = input(\"Enter the question: \")\n",
        "  if(question == \"end\"):\n",
        "    print(\"Good Bye!\")\n",
        "    break\n",
        "  \n",
        "  else:\n",
        "    q_type = get_q_type(question)\n",
        "    \n",
        "    try:\n",
        "      model_var = wh_dict[q_type]\n",
        "      model = model_var(corpus)\n",
        "\n",
        "    except:\n",
        "      print(\"Only WH questions are allowed (who, what, when, where, why)\")\n",
        "      continue\n",
        "    \n",
        "    print(\"Q: \", question)\n",
        "    \n",
        "    ans_sent = model.get_answer(question)     \n",
        "    ans = model.extract_answer(ans_sent, question)\n",
        "\n",
        "    print(\"A: \", ans_sent)  # ans_sent is the sentence containing the answer\n",
        "    print(\"Ans: \", ans)     # ans is the extracted answer from the ans_sent (Sometimes, ans maybe the same as ans_sent)\n",
        "    print(\"------------------------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the question: What is the name of the satellite that measured the amount of dust?\n",
            "Q:  What is the name of the satellite that measured the amount of dust?\n",
            "A:  nasa s calipso satellite has measured the amount of dust transported by wind from the sahara to the amazon an average 182 million tons of dust are windblown out of the sahara each year at 15 degrees west longitude across 1 600 miles 2 600 km over the atlantic ocean some dust falls into the atlantic then at 35 degrees west longitude at the eastern coast of south america 27 7 million tons 15 of dust fall over the amazon basin 132 million tons of dust remain in the air 43 million tons of dust are windblown and falls on the caribbean sea past 75 degrees west longitude \n",
            "Ans:  nasa calipso transported wind sahara amazon average 182 million tons windblown sahara year 15 degrees west longitude across 1 600 miles 2 600 km atlantic ocean falls atlantic 35 degrees west longitude eastern coast south america 27 7 million tons 15 fall amazon basin 132 million tons remain air 43 million tons windblown falls caribbean sea past 75 degrees west longitude \n",
            "------------------------------------------------------------\n",
            "Enter the question: What did the analysis from the sediment deposits indicate?\n",
            "Q:  What did the analysis from the sediment deposits indicate?\n",
            "A:  analyses of sediment deposits from amazon basin paleolakes and from the amazon fan indicate that rainfall in the basin during the lgm was lower than for the present and this was almost certainly associated with reduced moist tropical vegetation cover in the basin \n",
            "Ans:  analyses amazon basin paleolakes amazon fan rainfall basin lgm lower present almost certainly associated reduced moist tropical vegetation cover basin \n",
            "------------------------------------------------------------\n",
            "Enter the question: Where is the majority of the rainforest contained?\n",
            "Q:  Where is the majority of the rainforest contained?\n",
            "A:  the majority of the forest is contained within brazil with 60 of the rainforest followed by peru with 13 colombia with 10 and with minor amounts in venezuela ecuador bolivia guyana suriname and french guiana \n",
            "Ans:  brazil\n",
            "------------------------------------------------------------\n",
            "Enter the question: What is the name of the book written by Archeologist Betty Meggers?\n",
            "Q:  What is the name of the book written by Archeologist Betty Meggers?\n",
            "A:  archeologist betty meggers was a prominent proponent of this idea as described in her book amazonia man and culture in a counterfeit paradise \n",
            "Ans:  prominent proponent idea described amazonia man culture counterfeit paradise \n",
            "------------------------------------------------------------\n",
            "Enter the question: What is the average plant biosmass?\n",
            "Q:  What is the average plant biosmass?\n",
            "A:  the average plant biomass is estimated at 356 47 tonnes per hectare \n",
            "Ans:  biomass estimated 356 47 tonnes per hectare \n",
            "------------------------------------------------------------\n",
            "Enter the question: When dating rocks, what is the absolute isotopic date applied to?\n",
            "Q:  When dating rocks, what is the absolute isotopic date applied to?\n",
            "A:  with isotopic dates it became possible to assign absolute ages to rock units and these absolute dates could be applied to fossil sequences in which there was datable material converting the old relative ages into new absolute ages \n",
            "Ans:  dates became possible assign ages rock units dates could fossil sequences datable material converting old relative ages new ages \n",
            "------------------------------------------------------------\n",
            "Enter the question: What are the three major types of rock?\n",
            "Q:  What are the three major types of rock?\n",
            "A:  there are three major types of rock igneous sedimentary and metamorphic \n",
            "Ans:  igneous sedimentary metamorphic \n",
            "------------------------------------------------------------\n",
            "Enter the question: What types of waves do seismologists use to image the interior of the Earth?\n",
            "Q:  What types of waves do seismologists use to image the interior of the Earth?\n",
            "A:  seismologists can use the arrival times of seismic waves in reverse to image the interior of the earth \n",
            "Ans:  arrival times seismic reverse \n",
            "------------------------------------------------------------\n",
            "Enter the question: What principle relates to the formation of faults and the age of the sequences through which they cut?\n",
            "Q:  What principle relates to the formation of faults and the age of the sequences through which they cut?\n",
            "A:  the principle of cross cutting relationships pertains to the formation of faults and the age of the sequences through which they cut \n",
            "Ans:  cross cutting relationships pertains \n",
            "------------------------------------------------------------\n",
            "Enter the question: Where do thrust faults form?\n",
            "Q:  Where do thrust faults form?\n",
            "A:  a july 23 2006 article in the uk newspaper the independent reported woods hole research center results showing that the forest in its present form could survive only three years of drought \n",
            "Ans:  uk\n",
            "------------------------------------------------------------\n",
            "Enter the question: end\n",
            "Good Bye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZKz9ghvDHw8"
      },
      "source": [
        "# **Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_HJvIdcIie-"
      },
      "source": [
        "def compute_f1(prediction, truth):\n",
        "    pred_tokens = Quarc.tokenize(prediction)\n",
        "    truth_tokens = Quarc.tokenize(truth)\n",
        "    \n",
        "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return int(pred_tokens == truth_tokens)\n",
        "    \n",
        "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "    \n",
        "    # if there are no common tokens then f1 = 0\n",
        "    if len(common_tokens) == 0:\n",
        "        return 0\n",
        "    \n",
        "    prec = len(common_tokens) / len(pred_tokens)\n",
        "    rec = len(common_tokens) / len(truth_tokens)\n",
        "    \n",
        "    return 2 * (prec * rec) / (prec + rec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6-A2TiGvHf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7f7ad0e-f89d-4657-8bc2-aa2c98510981"
      },
      "source": [
        "questions = [\"What is the name of the satellite that measured the amount of dust?\",\n",
        "\"What did the analysis from the sediment deposits indicate?\",\n",
        "\"Where is the majority of the rainforest contained?\",\n",
        "\"What is the name of the book written by Archeologist Betty Meggers?\",\n",
        "\"What is the average plant biosmass?\",\n",
        "\"When dating rocks, what is the absolute isotopic date applied to?\",\n",
        "\"What are the three major types of rock?\",\n",
        "\"What types of waves do seismologists use to image the interior of the Earth?\",\n",
        "\"What principle relates to the formation of faults and the age of the sequences through which they cut?\",\n",
        "\"Where do thrust faults form?\"]\n",
        "\n",
        "ground_truth = [\"CALIPSO\",\n",
        "\"rainfall in the basin during the LGM was lower than for the present\",\n",
        "\"Brazil\",\n",
        "\"Amazonia: Man and Culture in a Counterfeit Paradise\",\n",
        "\"356 ± 47 tonnes per hectare\",\n",
        "\"fossil sequences\",\n",
        "\"igneous, sedimentary, and metamorphic\",\n",
        "\"seismic waves\",\n",
        "\"he principle of cross-cutting relationships\",\n",
        "\"In the shallow crust\"]\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for i,question in enumerate(questions):\n",
        "  q_type = get_q_type(question)\n",
        "\n",
        "  model_var = wh_dict[q_type]\n",
        "  model = model_var(corpus)\n",
        "\n",
        "  ans_sent = model.get_answer(question)\n",
        "  ans = model.extract_answer(ans_sent, question)\n",
        "  predictions.append(ans)\n",
        "\n",
        "f1_score = 0\n",
        "best_f1_score = 0\n",
        "for i, prediction in enumerate(predictions):\n",
        "  cur = compute_f1(prediction, ground_truth[i])\n",
        "  best_f1_score = max(cur, best_f1_score)\n",
        "  f1_score += cur\n",
        "\n",
        "avg_f1_score = f1_score/len(predictions)\n",
        "\n",
        "print(\"Average F1 score: \", avg_f1_score)\n",
        "print(\"Best F1 score: \", best_f1_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average F1 score:  0.5253686635944701\n",
            "Best F1 score:  1.0\n"
          ]
        }
      ]
    }
  ]
}